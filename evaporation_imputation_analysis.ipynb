{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"evaporation_imputation_analysis.ipynb","provenance":[],"authorship_tag":"ABX9TyP/OFajhngDvt80OkvXJOZ/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuT-XId4SRlY","executionInfo":{"status":"ok","timestamp":1639948674575,"user_tz":360,"elapsed":22029,"user":{"displayName":"Praveen Ravirathinam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13137473204960764107"}},"outputId":"b4b95dfa-e15a-4cca-d1ba-c005e36fc59d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"./drive/My Drive/Data Mining Project\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Data Mining Project\n"]}]},{"cell_type":"code","metadata":{"id":"RM9AdMYuSVDt"},"source":["import numpy as np \n","import pandas as pd \n","import glob\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, plot_confusion_matrix\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","np.set_printoptions(precision=3,suppress=True,linewidth=1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NS7k3OkSmLV"},"source":["## define function for metrics\n","def return_metrics(y_true, y_pred):\n","    print(classification_report(y_true,y_pred))\n","    \n","    cm = confusion_matrix(y_true,y_pred)\n","    print('TN : True Negative {}'.format(cm[0,0]))\n","    print('FP : False Positive {}'.format(cm[0,1]))\n","    print('FN : False Negative {}'.format(cm[1,0]))\n","    print('TP : True Positive {}'.format(cm[1,1]))\n","    print('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\n","    print('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))\n","    \n","    return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rui3kkOruEzE"},"source":["from sklearn.model_selection import RepeatedKFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","## function for model fits\n","\n","def fit_models(train_model_data,train_model_labels,test_model_data,test_model_labels,cv,metrics_array):\n","  list_scores = []\n","  ## KNN\n","  knn = KNeighborsClassifier(n_neighbors=10)\n","  knn.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(knn, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nKNN\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = knn.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[0:2,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[0:2,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[0:2,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Decision Tree\n","  dtc = DecisionTreeClassifier()\n","  dtc.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(dtc, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nDTC\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = dtc.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[2:4,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[2:4,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[2:4,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Naive Bayes\n","  nb = GaussianNB()\n","  nb.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(nb, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nNB\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = nb.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[4:6,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[4:6,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[4:6,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Logistic Regression\n","  log_lr = LogisticRegression(random_state = 42)\n","  log_lr.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(log_lr, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nLOG REG\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = log_lr.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[6:8,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[6:8,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[6:8,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Random Forest\n","  rfc = RandomForestClassifier(random_state = 42)\n","  rfc.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(rfc, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nRAND FOREST\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = rfc.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[8:10,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[8:10,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[8:10,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  print(list_scores)\n","  print('\\n METRICS ARRAY \\n')\n","  print(metrics_array)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = pd.read_csv('train.csv')\n","val_data = pd.read_csv('val.csv')\n","train_data = pd.concat([train_data,val_data])\n","test_data = pd.read_csv('test.csv')\n","print(train_data.shape)\n","print(test_data.shape)\n","evap_data = pd.read_csv('dataset.csv')\n","print(evap_data.shape)\n","feature_list = ['Location','MinTemp','MaxTemp','Rainfall','WindGustDir','WindGustSpeed','WindDir9am','WindDir3pm','WindSpeed9am','WindSpeed3pm',\n","              'Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Cloud9am','Cloud3pm','Temp9am','Temp3pm','RainToday','Sunshine','Date','RainTomorrow']\n","new_df = pd.merge(train_data, evap_data,  how='left', left_on= feature_list, right_on = feature_list)\n","new_df = new_df.drop('Unnamed: 0',axis=1) \n","new_df = new_df.drop('Evaporation_x',axis=1) \n","new_df = new_df.rename(columns={'Evaporation_y': 'Evaporation'})\n","new_df = new_df[test_data.columns]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xA5eQwZ0jtq-","executionInfo":{"status":"ok","timestamp":1639950661551,"user_tz":360,"elapsed":632,"user":{"displayName":"Praveen Ravirathinam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13137473204960764107"}},"outputId":"cff74f09-4f80-4c17-9b4c-9391ca500f23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(106644, 23)\n","(35549, 23)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dqdbh_V4TKct","executionInfo":{"status":"ok","timestamp":1639951050214,"user_tz":360,"elapsed":380213,"user":{"displayName":"Praveen Ravirathinam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13137473204960764107"}},"outputId":"ae19e886-c4be-417a-a9c2-e3c15e0a6304"},"source":["from sklearn.impute import SimpleImputer, KNNImputer\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","simp_median = SimpleImputer(strategy = 'median')\n","simp_mode = SimpleImputer(strategy = 'most_frequent')\n","knn_imp = KNNImputer(n_neighbors = 5)\n","\n","over_sm = SMOTE(sampling_strategy = 'minority', random_state = 1)\n","under = RandomUnderSampler(sampling_strategy = 1)\n","\n","def impute(X, imp, num_cols, cat_cols):\n","  imp[0].fit(X[num_cols])\n","  imp[1].fit(X[cat_cols])\n","  return imp\n","\n","# GET TRAIN DATA IN SHAPE\n","feature_list = ['Location','MinTemp','MaxTemp','Rainfall','WindGustDir','WindGustSpeed','WindDir9am','WindDir3pm','WindSpeed9am','WindSpeed3pm',\n","              'Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Cloud9am','Cloud3pm','Temp9am','Temp3pm','RainToday']\n","\n","train_data = new_df\n","test_data = pd.read_csv('test.csv')\n","print(train_data.shape)\n","print(test_data.shape)\n","\n","\n","print(train_data.shape)\n","\n","## Remove date column\n","train_data.drop('Date', axis = 1, inplace  =True)\n","test_data.drop('Date', axis = 1, inplace  =True)\n","train_data.drop('Location', axis = 1, inplace  =True)\n","test_data.drop('Location', axis = 1, inplace  =True)\n","\n","#fetching the numerical and categorical columns\n","obj = train_data.drop('RainTomorrow', axis = 1).select_dtypes(include = ['O'])\n","num_cols = list(set(train_data.drop('RainTomorrow', axis = 1).columns) - set(obj.columns))\n","obj_cols = list(set(obj.columns))\n","\n","## convert object columns to labels\n","def encode_labels(data,c1):\n","    from sklearn import preprocessing\n","    # label_encoder object knows how to understand word labels.\n","    label_encoder = preprocessing.LabelEncoder()\n","\n","    # Encode labels in column 'species'.\n","    data[c1]= label_encoder.fit_transform(data[c1].astype(str))\n","\n","    data[c1].unique()\n","    return data\n","\n","column_names = list(train_data.columns)\n","for column_name in column_names:\n","  if train_data[column_name].dtype == object:\n","    train_data = encode_labels(train_data,column_name)\n","\n","## get labels and drop label column from \n","train_data_x = train_data.drop('RainTomorrow', axis = 1)\n","train_labels = train_data['RainTomorrow']\n","\n","## convert object columns to labels\n","column_names_test = list(test_data.columns)\n","for column_name in column_names_test:\n","  if test_data[column_name].dtype == object:\n","    test_data = encode_labels(test_data,column_name)\n","\n","## get labels and drop label column from \n","test_data_x = test_data.drop('RainTomorrow', axis = 1)\n","test_labels = test_data['RainTomorrow']\n","\n","## impute using median for numerical data and mode for categorical data\n","train_data_x_imp = train_data_x\n","test_data_x_imp = test_data_x\n","\n","imp = impute(train_data_x, [simp_median, simp_mode], num_cols, obj_cols)\n","train_data_x_imp[num_cols] = imp[0].transform(train_data_x[num_cols])\n","test_data_x_imp[num_cols] = imp[0].transform(test_data_x[num_cols])\n","\n","train_data_x_imp[obj_cols] = imp[1].transform(train_data_x[obj_cols])\n","test_data_x_imp[obj_cols] = imp[1].transform(test_data_x[obj_cols])\n","\n","# oversampling the minority class mean mode imputation\n","train_data_x_over, train_labels_over = over_sm.fit_resample(train_data_x_imp, train_labels)\n","\n","##undersampling the majority class mean mode imputation\n","train_data_x_under, train_labels_under = under.fit_resample(train_data_x_imp, train_labels)\n","\n","## impute using KNN Imputer\n","train_data_x_imp_knn = knn_imp.fit_transform(train_data_x)\n","test_data_x_imp_knn = knn_imp.transform(test_data_x)\n","\n","# ## oversampling the minority class knn imputation\n","train_data_x_over_knn, train_labels_over_knn = over_sm.fit_resample(train_data_x_imp_knn, train_labels)\n","\n","# ##undersampling the majority class knn imputation\n","train_data_x_under_knn, train_labels_under_knn = under.fit_resample(train_data_x_imp_knn, train_labels)\n","\n","\n","print('TRAIN: ',train_data_x.shape,train_labels.shape)\n","print('TEST: ',test_data_x.shape,test_labels.shape)\n","\n","print('TRAIN: ',train_data_x_imp_knn.shape,train_labels.shape)\n","print('TEST: ',test_data_x_imp_knn.shape,test_labels.shape)\n","\n","# KNN imputation\n","scaler = StandardScaler()\n","train_data_x_scaled = scaler.fit_transform(train_data_x_imp_knn)\n","test_data_x_scaled = scaler.transform(test_data_x_imp_knn)\n","\n","metrics_array = np.zeros((10,3))\n","train_model_data = train_data_x_scaled\n","train_model_labels = train_labels\n","test_model_data = test_data_x_scaled\n","test_model_labels = test_labels\n","cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)\n","\n","fit_models(train_model_data,train_model_labels,test_model_data,test_model_labels,cv,metrics_array)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(106644, 23)\n","(35549, 23)\n","(106644, 23)\n","TRAIN:  (106644, 21) (106644,)\n","TEST:  (35549, 21) (35549,)\n","TRAIN:  (106644, 21) (106644,)\n","TEST:  (35549, 21) (35549,)\n","\n","KNN [0.569 0.574 0.568 0.57  0.569] \n"," 0.5698379403741748 0.0021749049666122404\n","\n","DTC [0.525 0.536 0.531 0.528 0.522] \n"," 0.5285654694699227 0.004836511853744316\n","\n","NB [0.586 0.587 0.584 0.581 0.578] \n"," 0.5833905925897929 0.003377979425413637\n","\n","LOG REG [0.582 0.582 0.589 0.586 0.583] \n"," 0.584289384796833 0.002559500451951697\n","\n","RAND FOREST [0.604 0.616 0.615 0.611 0.61 ] \n"," 0.6110798813318532 0.004306782524210166\n","[0.5698379403741748, 0.0021749049666122404, 0.5285654694699227, 0.004836511853744316, 0.5833905925897929, 0.003377979425413637, 0.584289384796833, 0.002559500451951697, 0.6110798813318532, 0.004306782524210166]\n","\n"," METRICS ARRAY \n","\n","[[0.863 0.933 0.897]\n"," [0.678 0.488 0.568]\n"," [0.867 0.855 0.861]\n"," [0.521 0.545 0.533]\n"," [0.886 0.859 0.872]\n"," [0.559 0.619 0.587]\n"," [0.867 0.946 0.904]\n"," [0.725 0.498 0.59 ]\n"," [0.872 0.954 0.912]\n"," [0.766 0.517 0.617]]\n"]}]}]}