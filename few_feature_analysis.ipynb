{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"few_feature_analysis.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOtvSfcTMz2xXkyzoBqgsFt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuT-XId4SRlY","executionInfo":{"status":"ok","timestamp":1639958604235,"user_tz":360,"elapsed":27257,"user":{"displayName":"Praveen Ravirathinam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13137473204960764107"}},"outputId":"e0d61d10-70e2-44ea-cef5-d7ef8794657e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"./drive/My Drive/Data Mining Project\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Data Mining Project\n"]}]},{"cell_type":"code","metadata":{"id":"RM9AdMYuSVDt"},"source":["import numpy as np \n","import pandas as pd \n","import glob\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, plot_confusion_matrix\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","np.set_printoptions(precision=3,suppress=True,linewidth=1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NS7k3OkSmLV"},"source":["## define function for metrics\n","def return_metrics(y_true, y_pred):\n","    print(classification_report(y_true,y_pred))\n","    \n","    cm = confusion_matrix(y_true,y_pred)\n","    print('TN : True Negative {}'.format(cm[0,0]))\n","    print('FP : False Positive {}'.format(cm[0,1]))\n","    print('FN : False Negative {}'.format(cm[1,0]))\n","    print('TP : True Positive {}'.format(cm[1,1]))\n","    print('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\n","    print('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))\n","    \n","    return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rui3kkOruEzE"},"source":["from sklearn.model_selection import RepeatedKFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","## function for model fits\n","\n","def fit_models(train_model_data,train_model_labels,test_model_data,test_model_labels,cv,metrics_array):\n","  list_scores = []\n","  ## KNN\n","  knn = KNeighborsClassifier(n_neighbors=10)\n","  knn.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(knn, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nKNN\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = knn.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[0:2,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[0:2,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[0:2,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Decision Tree\n","  dtc = DecisionTreeClassifier()\n","  dtc.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(dtc, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nDTC\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = dtc.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[2:4,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[2:4,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[2:4,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Naive Bayes\n","  nb = GaussianNB()\n","  nb.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(nb, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nNB\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = nb.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[4:6,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[4:6,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[4:6,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Logistic Regression\n","  log_lr = LogisticRegression(random_state = 42)\n","  log_lr.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(log_lr, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nLOG REG\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = log_lr.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[6:8,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[6:8,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[6:8,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Random Forest\n","  rfc = RandomForestClassifier(random_state = 42)\n","  rfc.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(rfc, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nRAND FOREST\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = rfc.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[8:10,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[8:10,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[8:10,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  print(list_scores)\n","  print('\\n METRICS ARRAY \\n')\n","  print(metrics_array)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kn2ZeEaSo1d","executionInfo":{"status":"ok","timestamp":1639958626753,"user_tz":360,"elapsed":5419,"user":{"displayName":"Praveen Ravirathinam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13137473204960764107"}},"outputId":"08207400-cd3f-40c8-e4bb-3eec3b5ab5ca"},"source":["full_data = pd.read_csv('weatherAUS.csv')\n","print(full_data.shape)\n","train_data = pd.read_csv('train.csv')\n","val_data = pd.read_csv('val.csv')\n","train_data = pd.concat([train_data,val_data])\n","test_data = pd.read_csv('test.csv')\n","print(train_data.shape)\n","print(test_data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(145460, 23)\n","(106644, 23)\n","(35549, 23)\n"]}]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer, KNNImputer\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","full_data = pd.read_csv('weatherAUS.csv')\n","print(full_data.shape)\n","train_data = pd.read_csv('train.csv')\n","val_data = pd.read_csv('val.csv')\n","train_data = pd.concat([train_data,val_data])\n","test_data = pd.read_csv('test.csv')\n","print(train_data.shape)\n","print(test_data.shape)\n","\n","simp_median = SimpleImputer(strategy = 'median')\n","simp_mode = SimpleImputer(strategy = 'most_frequent')\n","knn_imp = KNNImputer(n_neighbors = 5)\n","\n","over_sm = SMOTE(sampling_strategy = 'minority', random_state = 1)\n","under = RandomUnderSampler(sampling_strategy = 1)\n","\n","def impute(X, imp, num_cols, cat_cols):\n","  imp[0].fit(X[num_cols])\n","  imp[1].fit(X[cat_cols])\n","  return imp\n","\n","# GET TRAIN DATA IN SHAPE\n","\n","feature_to_remove_list = ['MinTemp','MaxTemp','Rainfall','WindGustDir','WindDir9am','Sunshine','WindSpeed3pm','WindSpeed9am',\n","                'Humidity9am','Pressure9am','Cloud9am','Temp9am','RainToday','Location']\n","for feature in feature_to_remove_list:\n","  \n","  feature_names = list(train_data.columns)\n","  print(feature,feature_names)\n","  if feature in feature_names:\n","    print(feature)\n","    train_data = train_data.drop(feature,axis=1)  \n","    test_data = test_data.drop(feature,axis=1)\n","\n","print(train_data.shape)\n","print(test_data.shape)\n","\n","## Remove date column\n","train_data.drop('Date', axis = 1, inplace  =True)\n","test_data.drop('Date', axis = 1, inplace  =True)\n","train_data.drop('Location', axis = 1, inplace  =True)\n","test_data.drop('Location', axis = 1, inplace  =True)\n","\n","#fetching the numerical and categorical columns\n","obj = train_data.drop('RainTomorrow', axis = 1).select_dtypes(include = ['O'])\n","num_cols = list(set(train_data.drop('RainTomorrow', axis = 1).columns) - set(obj.columns))\n","obj_cols = list(set(obj.columns))\n","print(obj_cols,num_cols)\n","## convert object columns to labels\n","def encode_labels(data,c1):\n","    from sklearn import preprocessing\n","    # label_encoder object knows how to understand word labels.\n","    label_encoder = preprocessing.LabelEncoder()\n","\n","    # Encode labels in column 'species'.\n","    data[c1]= label_encoder.fit_transform(data[c1].astype(str))\n","\n","    data[c1].unique()\n","    return data\n","\n","column_names = list(train_data.columns)\n","for column_name in column_names:\n","  if train_data[column_name].dtype == object:\n","    train_data = encode_labels(train_data,column_name)\n","\n","## get labels and drop label column from \n","train_data_x = train_data.drop('RainTomorrow', axis = 1)\n","train_labels = train_data['RainTomorrow']\n","\n","## convert object columns to labels\n","column_names_test = list(test_data.columns)\n","for column_name in column_names_test:\n","  if test_data[column_name].dtype == object:\n","    test_data = encode_labels(test_data,column_name)\n","\n","## get labels and drop label column from \n","test_data_x = test_data.drop('RainTomorrow', axis = 1)\n","test_labels = test_data['RainTomorrow']\n","\n","## impute using median for numerical data and mode for categorical data\n","train_data_x_imp = train_data_x\n","test_data_x_imp = test_data_x\n","\n","imp = impute(train_data_x, [simp_median, simp_mode], num_cols, obj_cols)\n","train_data_x_imp[num_cols] = imp[0].transform(train_data_x[num_cols])\n","test_data_x_imp[num_cols] = imp[0].transform(test_data_x[num_cols])\n","\n","train_data_x_imp[obj_cols] = imp[1].transform(train_data_x[obj_cols])\n","test_data_x_imp[obj_cols] = imp[1].transform(test_data_x[obj_cols])\n","\n","# oversampling the minority class mean mode imputation\n","train_data_x_over, train_labels_over = over_sm.fit_resample(train_data_x_imp, train_labels)\n","\n","##undersampling the majority class mean mode imputation\n","train_data_x_under, train_labels_under = under.fit_resample(train_data_x_imp, train_labels)\n","\n","## impute using KNN Imputer\n","train_data_x_imp_knn = knn_imp.fit_transform(train_data_x)\n","test_data_x_imp_knn = knn_imp.transform(test_data_x)\n","\n","# ## oversampling the minority class knn imputation\n","train_data_x_over_knn, train_labels_over_knn = over_sm.fit_resample(train_data_x_imp_knn, train_labels)\n","\n","# ##undersampling the majority class knn imputation\n","train_data_x_under_knn, train_labels_under_knn = under.fit_resample(train_data_x_imp_knn, train_labels)\n","\n","\n","print('TRAIN: ',train_data_x.shape,train_labels.shape)\n","print('TEST: ',test_data_x.shape,test_labels.shape)\n","\n","print('TRAIN: ',train_data_x_imp_knn.shape,train_labels.shape)\n","print('TEST: ',test_data_x_imp_knn.shape,test_labels.shape)\n","\n","# KNN imputation\n","scaler = StandardScaler()\n","train_data_x_scaled = scaler.fit_transform(train_data_x_imp_knn)\n","test_data_x_scaled = scaler.transform(test_data_x_imp_knn)\n","\n","metrics_array = np.zeros((10,3))\n","train_model_data = train_data_x_scaled\n","train_model_labels = train_labels\n","test_model_data = test_data_x_scaled\n","test_model_labels = test_labels\n","cv = RepeatedKFold(n_splits=3, n_repeats=1, random_state=1)\n","\n","fit_models(train_model_data,train_model_labels,test_model_data,test_model_labels,cv,metrics_array)"],"metadata":{"id":"jAqbPaUVVFC_"},"execution_count":null,"outputs":[]}]}