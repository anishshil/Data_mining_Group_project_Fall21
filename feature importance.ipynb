{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"feature importance.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOkaXGxAXZcOrp4LfI2UKkE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuT-XId4SRlY","executionInfo":{"status":"ok","timestamp":1638830545331,"user_tz":360,"elapsed":19823,"user":{"displayName":"Praveen Ravirathinam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13137473204960764107"}},"outputId":"3b1fb299-f158-4a41-f67f-32f821d6b3ab"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"./drive/My Drive/Data Mining Project\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Data Mining Project\n"]}]},{"cell_type":"code","metadata":{"id":"RM9AdMYuSVDt"},"source":["import numpy as np \n","import pandas as pd \n","import glob\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, plot_confusion_matrix\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","np.set_printoptions(precision=3,suppress=True,linewidth=1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NS7k3OkSmLV"},"source":["## define function for metrics\n","def return_metrics(y_true, y_pred):\n","    print(classification_report(y_true,y_pred))\n","    \n","    cm = confusion_matrix(y_true,y_pred)\n","    print('TN : True Negative {}'.format(cm[0,0]))\n","    print('FP : False Positive {}'.format(cm[0,1]))\n","    print('FN : False Negative {}'.format(cm[1,0]))\n","    print('TP : True Positive {}'.format(cm[1,1]))\n","    print('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\n","    print('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))\n","    \n","    return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rui3kkOruEzE"},"source":["from sklearn.model_selection import RepeatedKFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","## function for model fits\n","\n","def fit_models(train_model_data,train_model_labels,test_model_data,test_model_labels,cv,metrics_array):\n","  list_scores = []\n","  ## KNN\n","  knn = KNeighborsClassifier(n_neighbors=10)\n","  knn.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(knn, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nKNN\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = knn.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[0:2,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[0:2,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[0:2,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Decision Tree\n","  dtc = DecisionTreeClassifier()\n","  dtc.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(dtc, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nDTC\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = dtc.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[2:4,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[2:4,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[2:4,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Naive Bayes\n","  nb = GaussianNB()\n","  nb.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(nb, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nNB\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = nb.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[4:6,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[4:6,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[4:6,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Logistic Regression\n","  log_lr = LogisticRegression(random_state = 42)\n","  log_lr.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(log_lr, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nLOG REG\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = log_lr.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[6:8,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[6:8,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[6:8,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  ## Random Forest\n","  rfc = RandomForestClassifier(random_state = 42)\n","  rfc.fit(train_model_data,train_model_labels)\n","  scores = cross_val_score(rfc, train_model_data, train_model_labels, scoring='f1', cv=cv, n_jobs=-1)\n","  print(\"\\nRAND FOREST\",scores,\"\\n\",np.mean(scores),np.std(scores))\n","  list_scores.append(np.mean(scores))\n","  list_scores.append(np.std(scores))\n","  pred_labels = rfc.predict(test_model_data)\n","  # return_metrics(test_model_labels,pred_labels)\n","  metrics_array[8:10,0] = precision_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[8:10,1] = recall_score(test_model_labels,pred_labels,average=None)\n","  metrics_array[8:10,2] = f1_score(test_model_labels,pred_labels,average=None)\n","\n","  print(list_scores)\n","  print('\\n METRICS ARRAY \\n')\n","  print(metrics_array)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kn2ZeEaSo1d","executionInfo":{"status":"ok","timestamp":1638830550208,"user_tz":360,"elapsed":2986,"user":{"displayName":"Praveen Ravirathinam","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13137473204960764107"}},"outputId":"4d956a92-376a-4b6a-c913-a6045e16e266"},"source":["full_data = pd.read_csv('weatherAUS.csv')\n","print(full_data.shape)\n","train_data = pd.read_csv('train.csv')\n","val_data = pd.read_csv('val.csv')\n","train_data = pd.concat([train_data,val_data])\n","test_data = pd.read_csv('test.csv')\n","print(train_data.shape)\n","print(test_data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(145460, 23)\n","(106644, 23)\n","(35549, 23)\n"]}]},{"cell_type":"code","metadata":{"id":"Dqdbh_V4TKct"},"source":["from sklearn.impute import SimpleImputer, KNNImputer\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","simp_median = SimpleImputer(strategy = 'median')\n","simp_mode = SimpleImputer(strategy = 'most_frequent')\n","knn_imp = KNNImputer(n_neighbors = 5)\n","\n","over_sm = SMOTE(sampling_strategy = 'minority', random_state = 1)\n","under = RandomUnderSampler(sampling_strategy = 1)\n","\n","def impute(X, imp, num_cols, cat_cols):\n","  imp[0].fit(X[num_cols])\n","  imp[1].fit(X[cat_cols])\n","  return imp\n","\n","# GET TRAIN DATA IN SHAPE\n","feature_list = ['MinTemp','MaxTemp','Rainfall','WindGustDir','WindGustSpeed','WindDir9am','WindDir3pm','WindSpeed9am','WindSpeed3pm',\n","                'Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Cloud9am','Cloud3pm','Temp9am','Temp3pm','RainToday','Evaporation','Sunshine']\n","\n","for feature in feature_list:\n","  print(\"\\n\\n\",feature)\n","  train_data = pd.read_csv('train.csv')\n","  val_data = pd.read_csv('val.csv')\n","  train_data = pd.concat([train_data,val_data])\n","  test_data = pd.read_csv('test.csv')\n","  print(train_data.shape)\n","  print(test_data.shape)\n","\n","  \n","  feature_names = list(train_data.columns)\n","  if feature in feature_names:\n","    train_data = train_data.drop(feature,axis=1)  \n","    test_data = test_data.drop(feature,axis=1)\n","\n","  print(train_data.shape)\n","\n","  ## Remove date column\n","  train_data.drop('Date', axis = 1, inplace  =True)\n","  # val_data.drop('Date', axis = 1, inplace  =True)\n","  test_data.drop('Date', axis = 1, inplace  =True)\n","  train_data.drop('Location', axis = 1, inplace  =True)\n","  test_data.drop('Location', axis = 1, inplace  =True)\n","\n","  #fetching the numerical and categorical columns\n","  obj = train_data.drop('RainTomorrow', axis = 1).select_dtypes(include = ['O'])\n","  num_cols = list(set(train_data.drop('RainTomorrow', axis = 1).columns) - set(obj.columns))\n","  obj_cols = list(set(obj.columns))\n","\n","  ## convert object columns to labels\n","  def encode_labels(data,c1):\n","      from sklearn import preprocessing\n","      # label_encoder object knows how to understand word labels.\n","      label_encoder = preprocessing.LabelEncoder()\n","  \n","      # Encode labels in column 'species'.\n","      data[c1]= label_encoder.fit_transform(data[c1].astype(str))\n","  \n","      data[c1].unique()\n","      return data\n","\n","  column_names = list(train_data.columns)\n","  for column_name in column_names:\n","    if train_data[column_name].dtype == object:\n","      train_data = encode_labels(train_data,column_name)\n","\n","  ## get labels and drop label column from \n","  train_data_x = train_data.drop('RainTomorrow', axis = 1)\n","  train_labels = train_data['RainTomorrow']\n","\n","  ## convert object columns to labels\n","  column_names_test = list(test_data.columns)\n","  for column_name in column_names_test:\n","    if test_data[column_name].dtype == object:\n","      test_data = encode_labels(test_data,column_name)\n","\n","  ## get labels and drop label column from \n","  test_data_x = test_data.drop('RainTomorrow', axis = 1)\n","  test_labels = test_data['RainTomorrow']\n","\n","  ## impute using median for numerical data and mode for categorical data\n","  train_data_x_imp = train_data_x\n","  test_data_x_imp = test_data_x\n","\n","  imp = impute(train_data_x, [simp_median, simp_mode], num_cols, obj_cols)\n","  train_data_x_imp[num_cols] = imp[0].transform(train_data_x[num_cols])\n","  test_data_x_imp[num_cols] = imp[0].transform(test_data_x[num_cols])\n","\n","  train_data_x_imp[obj_cols] = imp[1].transform(train_data_x[obj_cols])\n","  test_data_x_imp[obj_cols] = imp[1].transform(test_data_x[obj_cols])\n","\n","  # oversampling the minority class mean mode imputation\n","  train_data_x_over, train_labels_over = over_sm.fit_resample(train_data_x_imp, train_labels)\n","\n","  ##undersampling the majority class mean mode imputation\n","  train_data_x_under, train_labels_under = under.fit_resample(train_data_x_imp, train_labels)\n","\n","  ## impute using KNN Imputer\n","  train_data_x_imp_knn = knn_imp.fit_transform(train_data_x)\n","  test_data_x_imp_knn = knn_imp.transform(test_data_x)\n","\n","  # ## oversampling the minority class knn imputation\n","  train_data_x_over_knn, train_labels_over_knn = over_sm.fit_resample(train_data_x_imp_knn, train_labels)\n","\n","  # ##undersampling the majority class knn imputation\n","  train_data_x_under_knn, train_labels_under_knn = under.fit_resample(train_data_x_imp_knn, train_labels)\n","\n","\n","  print('TRAIN: ',train_data_x.shape,train_labels.shape)\n","  print('TEST: ',test_data_x.shape,test_labels.shape)\n","\n","  print('TRAIN: ',train_data_x_imp_knn.shape,train_labels.shape)\n","  print('TEST: ',test_data_x_imp_knn.shape,test_labels.shape)\n","\n","  # KNN imputation\n","  scaler = StandardScaler()\n","  train_data_x_scaled = scaler.fit_transform(train_data_x_imp_knn)\n","  test_data_x_scaled = scaler.transform(test_data_x_imp_knn)\n","\n","  metrics_array = np.zeros((10,3))\n","  train_model_data = train_data_x_scaled\n","  train_model_labels = train_labels\n","  test_model_data = test_data_x_scaled\n","  test_model_labels = test_labels\n","  cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)\n","\n","  fit_models(train_model_data,train_model_labels,test_model_data,test_model_labels,cv,metrics_array)"],"execution_count":null,"outputs":[]}]}